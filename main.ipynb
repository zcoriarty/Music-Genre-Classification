{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "# Preprocesing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# model libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, StringLookup, IntegerLookup, CategoryEncoding\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "# put data into a dataframe\n",
    "df = pd.read_csv(\"Data/features_30_sec.csv\")\n",
    "\n",
    "# remove unnessary columns\n",
    "input_data = df.drop(labels=['filename', 'length'], axis=1)\n",
    "\n",
    "# get all of the columns for specifying network features\n",
    "columns = df.columns\n",
    "features = columns[2:]\n",
    "num_of_features = len(features[2:])\n",
    "\n",
    "# one hot encoding for string labels\n",
    "vocab = ['disco', 'metal', 'classical', 'reggae', 'blues', 'rock', 'hiphop', 'jazz', 'pop', 'country']\n",
    "input_data['label'] = pd.get_dummies(input_data['label'], vocab)\n",
    "\n",
    "# split data into training, testing, and validation\n",
    "train, test = train_test_split(input_data[features], test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "\n",
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "\tdataframe = dataframe.copy()\n",
    "\tlabels = dataframe.pop('label')\n",
    "\tds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "\tif shuffle:\n",
    "\t\tds = ds.shuffle(buffer_size=len(dataframe))\n",
    "\tds = ds.batch(batch_size)\n",
    "\tds = ds.prefetch(batch_size)\n",
    "\treturn ds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "\n",
    "# normalize the data so mu is 0 and std dev is 1\n",
    "def get_normalization_layer(name, dataset):\n",
    "\t# Create a Normalization layer for our feature.\n",
    "\tnormalizer = tf.keras.layers.Normalization(axis=None)\n",
    "\n",
    "\t# Prepare a Dataset that only yields our feature.\n",
    "\tfeature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "\t# Learn the statistics of the data.\n",
    "\tnormalizer.adapt(feature_ds)\n",
    "\n",
    "\treturn normalizer"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([ 1.3380439 , -0.5863977 , -1.2642847 ,  0.25080043, -1.734972  ],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert the train, validation, and test set to a tensor\n",
    "batch_size = 256\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "# for numerical features, dont include label in the last column\n",
    "for header in features[:-1]:\n",
    "\tnumeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "\tnormalization_layer = get_normalization_layer(header, train_ds)\n",
    "\tencoded_numeric_col = normalization_layer(numeric_col)\n",
    "\tall_inputs.append(numeric_col)\n",
    "\tencoded_features.append(encoded_numeric_col)\n",
    "\n",
    "# concat the tensors of each feature for network input \n",
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "\n",
    "# neural network\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "model = tf.keras.Model(all_inputs, output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "# fit the model and validate it\n",
    "model.fit(train_ds, epochs=10, validation_data=val_ds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.0668 - accuracy: 0.9797 - val_loss: 0.2275 - val_accuracy: 0.9062\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0713 - accuracy: 0.9719 - val_loss: 0.2271 - val_accuracy: 0.9062\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0596 - accuracy: 0.9812 - val_loss: 0.2270 - val_accuracy: 0.9062\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0658 - accuracy: 0.9766 - val_loss: 0.2273 - val_accuracy: 0.9062\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0556 - accuracy: 0.9797 - val_loss: 0.2279 - val_accuracy: 0.9062\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0632 - accuracy: 0.9844 - val_loss: 0.2286 - val_accuracy: 0.9062\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0524 - accuracy: 0.9875 - val_loss: 0.2296 - val_accuracy: 0.9062\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0616 - accuracy: 0.9812 - val_loss: 0.2304 - val_accuracy: 0.9062\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0616 - accuracy: 0.9766 - val_loss: 0.2307 - val_accuracy: 0.9062\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0600 - accuracy: 0.9797 - val_loss: 0.2307 - val_accuracy: 0.9125\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18c6c7ca0>"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "# evaluate the model with test data\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1164 - accuracy: 0.9450\n",
      "Accuracy 0.9449999928474426\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}